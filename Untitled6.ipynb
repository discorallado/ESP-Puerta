{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy24slKROqnfNJcoPcuDJI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/discorallado/ESP-Puerta/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook: Scraper inteligente para **vitelenergia.com** (Google Colab)\n",
        "Este notebook extrae el catálogo completo de https://vitelenergia.com/ y guarda:\n",
        "- CSV: `vitelenergia_catalogo.csv`\n",
        "- JSON: `vitelenergia_catalogo.json`\n",
        "- SQLite: `vitelenergia.db` (tabla `productos`)\n",
        "\n",
        "Columnas: `sku, nombre, categoria, precio_sin_dcto, precio_con_dcto, url, url_ficha_tecnica`.\n",
        "\n",
        "Características:\n",
        "- Guardado incremental (`progress.json`) y reanudable.\n",
        "- Prevención de duplicados por SKU y por URL.\n",
        "- Modo rápido (`FAST_MODE`) para listar sin visitar cada producto.\n",
        "- IA local ligera basada en heurísticas para extraer precios, SKU y ficha técnica."
      ],
      "metadata": {
        "id": "guzbGYpO6EAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1 - Instalar dependencias\n",
        "!pip install -q requests beautifulsoup4 lxml pandas tenacity tqdm\n",
        "print(\"Dependencias instaladas.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngv165d76HFb",
        "outputId": "08eb3974-584b-4f04-b12c-32683ae2c91f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencias instaladas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H8tNn5tD6DYu"
      },
      "outputs": [],
      "source": [
        "# Celda 2 - Imports y configuración general\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import sqlite3\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# CONFIG\n",
        "BASE_URL = \"https://vitelenergia.com\"\n",
        "OUTPUT_CSV = \"vitelenergia_catalogo.csv\"\n",
        "OUTPUT_JSON = \"vitelenergia_catalogo.json\"\n",
        "OUTPUT_DB = \"vitelenergia.db\"\n",
        "PROGRESS_FILE = \"progress.json\"\n",
        "\n",
        "# Control de ejecución\n",
        "FAST_MODE = False         # True = solo listado rápido (no visita cada producto)\n",
        "SAVE_EVERY_N = 20        # guardar progreso cada N productos\n",
        "REQUEST_DELAY_MIN = 0.6\n",
        "REQUEST_DELAY_MAX = 1.4\n",
        "MAX_PAGES_PER_CATEGORY = 200\n",
        "\n",
        "HEADERS_POOL = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/120.0 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/605.1.15 \"\n",
        "    \"(KHTML, like Gecko) Version/16.6 Safari/605.1.15\",\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/118.0 Safari/537.36\"\n",
        "]\n",
        "\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 3 - Utilidades de red y reintentos\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_random_exponential(min=1, max=4))\n",
        "def fetch_html(url: str, timeout: int = 12) -> str:\n",
        "    headers = {\"User-Agent\": random.choice(HEADERS_POOL)}\n",
        "    resp = requests.get(url, headers=headers, timeout=timeout)\n",
        "    resp.raise_for_status()\n",
        "    # Respectful delay\n",
        "    time.sleep(random.uniform(REQUEST_DELAY_MIN, REQUEST_DELAY_MAX))\n",
        "    return resp.text\n",
        "\n",
        "def clean_text(s: Optional[str]) -> str:\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    return re.sub(r'\\s+', ' ', s).strip()"
      ],
      "metadata": {
        "id": "pRxM8gjf6Nvk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 4 - IA ligera (heurísticas) para precios, SKU y ficha técnica\n",
        "def parse_price_candidates(text: str) -> List[float]:\n",
        "    \"\"\"\n",
        "    Extrae candidatos numéricos desde texto y normaliza a float.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    text = text.replace('\\xa0',' ')\n",
        "    # extraer patrones numéricos con posibles separadores\n",
        "    matches = re.findall(r'(?:(?:\\$|\\b)(?:\\s?))?([\\d]{1,3}(?:[.,\\s]\\d{3})*(?:[.,]\\d{1,2})?)', text)\n",
        "    nums = []\n",
        "    for m in matches:\n",
        "        s = m.strip().replace(' ', '').replace('\\u200b','')\n",
        "        if s.count('.') and s.count(','):\n",
        "            if s.find('.') < s.find(','):\n",
        "                s = s.replace('.','').replace(',','.')\n",
        "            else:\n",
        "                s = s.replace(',','')\n",
        "        elif s.count(',') and not s.count('.'):\n",
        "            parts = s.split(',')\n",
        "            if len(parts[-1]) in (1,2):\n",
        "                s = ''.join(parts[:-1]).replace('.','') + '.' + parts[-1]\n",
        "            else:\n",
        "                s = ''.join(parts)\n",
        "        else:\n",
        "            s = s.replace(',','').replace('.','')\n",
        "        s = re.sub(r'[^0-9\\.]', '', s)\n",
        "        if not s:\n",
        "            continue\n",
        "        try:\n",
        "            val = float(s)\n",
        "            nums.append(val)\n",
        "        except:\n",
        "            continue\n",
        "    nums_unique = sorted(list(set(nums)))\n",
        "    return nums_unique\n",
        "\n",
        "def select_price_pair(candidates: List[float]) -> Tuple[Optional[float], Optional[float]]:\n",
        "    \"\"\"\n",
        "    Mayor -> precio sin descuento, menor -> precio con descuento\n",
        "    \"\"\"\n",
        "    if not candidates:\n",
        "        return None, None\n",
        "    if len(candidates) == 1:\n",
        "        return candidates[-1], None\n",
        "    return candidates[-1], candidates[0]\n",
        "\n",
        "def detect_sku_from_text(text: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Detecta SKU a partir de patrones comunes.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "    patterns = [\n",
        "        r'(?:sku|sku:|sku\\.|cod|cod\\.|codigo|código|ref|ref\\.|referencia|ref:)\\s*[:\\-]?\\s*([A-Za-z0-9\\-_\\/]+)',\n",
        "        r'Código[:\\s]*([A-Za-z0-9\\-_\\/]+)'\n",
        "    ]\n",
        "    for p in patterns:\n",
        "        m = re.search(p, text, re.I)\n",
        "        if m:\n",
        "            return m.group(1).strip()\n",
        "    return None\n",
        "\n",
        "def find_datasheet_link(soup: BeautifulSoup, base_url: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Busca enlaces a PDF o a textos que indiquen 'Ficha técnica' o 'Datasheet'.\n",
        "    \"\"\"\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        href = a['href'].strip()\n",
        "        text = a.get_text(\" \", strip=True).lower()\n",
        "        if '.pdf' in href.lower() or 'ficha' in text or 'datasheet' in text or 'descargar ficha' in text:\n",
        "            return urljoin(base_url, href)\n",
        "    # buscar secciones con clase 'download' 'ficha' etc.\n",
        "    for sec in soup.find_all(class_=re.compile(r'(download|descarga|ficha|datasheet)', re.I)):\n",
        "        a = sec.find('a', href=True)\n",
        "        if a:\n",
        "            return urljoin(base_url, a['href'])\n",
        "    return None"
      ],
      "metadata": {
        "id": "cd4ofhyW6O0r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 5 - Detección de categorías desde el HEADER y construcción de lista\n",
        "def extract_header_categories(home_html: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Extrae (nombre, url) de categorías desde el header/nav del home.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(home_html, \"lxml\")\n",
        "    categories = []\n",
        "    navs = soup.find_all('nav') or []\n",
        "    candidates = navs if navs else ([soup.find('header')] if soup.find('header') else [])\n",
        "    seen = set()\n",
        "    for nav in candidates:\n",
        "        if not nav:\n",
        "            continue\n",
        "        for a in nav.find_all('a', href=True):\n",
        "            name = clean_text(a.get_text())\n",
        "            href = a['href']\n",
        "            if not name:\n",
        "                continue\n",
        "            if href.startswith('#') or 'javascript:' in href.lower():\n",
        "                continue\n",
        "            full = urljoin(BASE_URL, href)\n",
        "            if full.rstrip('/') == BASE_URL.rstrip('/'):\n",
        "                continue\n",
        "            skip = ['contacto','blog','login','carrito','cart','faq','nosotros','empresa','servicios']\n",
        "            if any(k in name.lower() for k in skip) or any(k in full.lower() for k in skip):\n",
        "                continue\n",
        "            key = (name.lower(), full)\n",
        "            if key in seen:\n",
        "                continue\n",
        "            seen.add(key)\n",
        "            categories.append((name, full))\n",
        "    return categories\n",
        "\n",
        "def build_category_tree(categories: List[Tuple[str,str]]) -> List[Tuple[str,str]]:\n",
        "    \"\"\"\n",
        "    Por ahora retorna lista plana; preparada para árbol.\n",
        "    \"\"\"\n",
        "    return categories"
      ],
      "metadata": {
        "id": "f9bLeYtj6PCy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 6 - Heurísticas para encontrar bloques de producto en página de categoría\n",
        "def find_product_blocks(soup: BeautifulSoup) -> List[BeautifulSoup]:\n",
        "    selectors = [\n",
        "        'li.product', '.product', '.product-item', '.woocommerce-product', '.product-card',\n",
        "        '.grid-item', '.product-loop-item', '.product-block'\n",
        "    ]\n",
        "    blocks = []\n",
        "    for sel in selectors:\n",
        "        found = soup.select(sel)\n",
        "        if found:\n",
        "            blocks.extend(found)\n",
        "    if not blocks:\n",
        "        for li in soup.find_all('li'):\n",
        "            if li.find('a', href=True) and li.find('img'):\n",
        "                blocks.append(li)\n",
        "    uniq = []\n",
        "    seen = set()\n",
        "    for b in blocks:\n",
        "        try:\n",
        "            key = (b.name, tuple(b.get('class') or []), (b.find('a')['href'] if b.find('a') and b.find('a').get('href') else ''))\n",
        "        except:\n",
        "            key = str(b)[:200]\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            uniq.append(b)\n",
        "    return uniq"
      ],
      "metadata": {
        "id": "GSZq8Sn56PG6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 7 - Scrape de página de producto (detallado)\n",
        "def scrape_product_page(product_url: str) -> Dict[str, Optional[str]]:\n",
        "    \"\"\"\n",
        "    Visita la página del producto y extrae:\n",
        "    sku, nombre, precio_sin_dcto, precio_con_dcto, url_ficha_tecnica\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        \"sku\": \"\",\n",
        "        \"nombre\": \"\",\n",
        "        \"precio_sin_dcto\": \"\",\n",
        "        \"precio_con_dcto\": \"\",\n",
        "        \"url\": product_url,\n",
        "        \"url_ficha_tecnica\": \"\"\n",
        "    }\n",
        "    try:\n",
        "        html = fetch_html(product_url)\n",
        "    except Exception as e:\n",
        "        return result\n",
        "\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    text_all = soup.get_text(\" \", strip=True)\n",
        "\n",
        "    # Nombre: h1 preferido\n",
        "    name = \"\"\n",
        "    h1 = soup.find('h1')\n",
        "    if h1 and h1.get_text(strip=True):\n",
        "        name = clean_text(h1.get_text())\n",
        "    else:\n",
        "        # buscar elementos con clase title/product\n",
        "        title_el = soup.find(attrs={\"class\": re.compile(r'(product|title|nombre|name|titulo)', re.I)})\n",
        "        if title_el and title_el.get_text(strip=True):\n",
        "            name = clean_text(title_el.get_text())\n",
        "        else:\n",
        "            name = clean_text(soup.title.string) if soup.title and soup.title.string else \"\"\n",
        "\n",
        "    result[\"nombre\"] = name\n",
        "\n",
        "    # SKU\n",
        "    sku = None\n",
        "    sku_el = soup.find(attrs={\"class\": re.compile(r'(sku|codigo|code|ref)', re.I)}) or soup.find(id=re.compile(r'(sku|codigo|code|ref)', re.I))\n",
        "    if sku_el and sku_el.get_text(strip=True):\n",
        "        sku = clean_text(sku_el.get_text())\n",
        "    if not sku:\n",
        "        sku = detect_sku_from_text(text_all)\n",
        "    if sku:\n",
        "        sku = re.sub(r'(?i)(sku|cod|codigo|ref|referencia)[:\\s\\-]*', '', sku).strip()\n",
        "        result[\"sku\"] = sku\n",
        "\n",
        "    # Precios: buscar contenedores con clase 'price' 'precio' y fallback a todo el texto\n",
        "    price_texts = []\n",
        "    price_containers = soup.find_all(class_=re.compile(r'(price|precio|amount|valor)', re.I))\n",
        "    for pc in price_containers:\n",
        "        price_texts.append(pc.get_text(\" \", strip=True))\n",
        "    # fallback: toda la página\n",
        "    price_texts.append(text_all)\n",
        "\n",
        "    candidates = []\n",
        "    for pt in price_texts:\n",
        "        candidates.extend(parse_price_candidates(pt))\n",
        "    candidates = sorted(list(set(candidates)))\n",
        "    p_before, p_after = select_price_pair(candidates)\n",
        "    if p_before is not None:\n",
        "        result[\"precio_sin_dcto\"] = str(int(p_before)) if float(p_before).is_integer() else str(p_before)\n",
        "    if p_after is not None:\n",
        "        result[\"precio_con_dcto\"] = str(int(p_after)) if float(p_after).is_integer() else str(p_after)\n",
        "\n",
        "    # Ficha técnica\n",
        "    ficha = find_datasheet_link(soup, product_url)\n",
        "    if ficha:\n",
        "        result[\"url_ficha_tecnica\"] = ficha\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "GhdVQK3k6PJ6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 8 - Guardado incremental (progress) y deduplicación (por SKU y URL)\n",
        "def load_progress() -> Dict:\n",
        "    if os.path.exists(PROGRESS_FILE):\n",
        "        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    # estructura:\n",
        "    return {\"collected\": [], \"by_sku\": {}, \"by_url\": {}, \"last_category_index\": 0, \"processed_count\": 0}\n",
        "\n",
        "def save_progress(progress: Dict):\n",
        "    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:\n",
        "        json.dump(progress, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def initialize_db(db_path: str = OUTPUT_DB):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS productos (\n",
        "            sku TEXT PRIMARY KEY,\n",
        "            nombre TEXT,\n",
        "            categoria TEXT,\n",
        "            precio_sin_dcto REAL,\n",
        "            precio_con_dcto REAL,\n",
        "            url TEXT,\n",
        "            url_ficha_tecnica TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def upsert_product_db(product: Dict, db_path: str = OUTPUT_DB):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "    # usar REPLACE INTO para simplificar (o INSERT OR REPLACE)\n",
        "    cur.execute(\"\"\"\n",
        "        INSERT OR REPLACE INTO productos(sku, nombre, categoria, precio_sin_dcto, precio_con_dcto, url, url_ficha_tecnica)\n",
        "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
        "    \"\"\", (\n",
        "        product.get(\"sku\") or product.get(\"url\"),\n",
        "        product.get(\"nombre\"),\n",
        "        product.get(\"categoria\"),\n",
        "        float(product.get(\"precio_sin_dcto\")) if product.get(\"precio_sin_dcto\") else None,\n",
        "        float(product.get(\"precio_con_dcto\")) if product.get(\"precio_con_dcto\") else None,\n",
        "        product.get(\"url\"),\n",
        "        product.get(\"url_ficha_tecnica\")\n",
        "    ))\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "PIKfBiXW6PMa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 9 - Scrape de categoría con modo FAST_MODE y control de paginación\n",
        "def scrape_category(category_name: str, category_url: str, progress: Dict, start_id: int = 1) -> Tuple[List[Dict], Dict]:\n",
        "    \"\"\"\n",
        "    Recorre una categoría (paginación), extrae enlaces y (opcional) visita cada producto.\n",
        "    Actualiza progress dict y devuelve lista de nuevos productos encontrados.\n",
        "    \"\"\"\n",
        "    print(f\"Scrapeando categoría: {category_name} -> {category_url}\")\n",
        "    new_products = []\n",
        "    to_visit_pages = [category_url]\n",
        "    visited_pages = set()\n",
        "    pages_crawled = 0\n",
        "\n",
        "    while to_visit_pages and pages_crawled < MAX_PAGES_PER_CATEGORY:\n",
        "        page_url = to_visit_pages.pop(0)\n",
        "        if page_url in visited_pages:\n",
        "            continue\n",
        "        visited_pages.add(page_url)\n",
        "        pages_crawled += 1\n",
        "        try:\n",
        "            html = fetch_html(page_url)\n",
        "        except Exception as e:\n",
        "            print(f\"  [!] Error al cargar {page_url}: {e}\")\n",
        "            continue\n",
        "        soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "        blocks = find_product_blocks(soup)\n",
        "        # fallback: enlaces con '/product' en href\n",
        "        if not blocks:\n",
        "            for a in soup.find_all('a', href=True):\n",
        "                href = a['href']\n",
        "                if '/product' in href or '/producto' in href or '/producto-' in href:\n",
        "                    blocks.append(a)\n",
        "\n",
        "        for b in blocks:\n",
        "            # obtener URL y nombre del bloque\n",
        "            a = b.find('a', href=True) if hasattr(b, 'find') else None\n",
        "            product_url = urljoin(page_url, a['href']) if a and a.get('href') else (a if isinstance(a,str) else None)\n",
        "            name_block = clean_text(a.get_text()) if a and a.get_text() else clean_text(b.get_text(\" \", strip=True)[:120])\n",
        "\n",
        "            if not product_url:\n",
        "                continue\n",
        "            # deduplicación por URL\n",
        "            if product_url in progress.get(\"by_url\", {}):\n",
        "                # ya procesado o listado\n",
        "                continue\n",
        "\n",
        "            if FAST_MODE:\n",
        "                # Modo rápido: solo tomar nombre, url y categoría\n",
        "                prod = {\n",
        "                    \"sku\": \"\",\n",
        "                    \"nombre\": name_block,\n",
        "                    \"categoria\": category_name,\n",
        "                    \"precio_sin_dcto\": \"\",\n",
        "                    \"precio_con_dcto\": \"\",\n",
        "                    \"url\": product_url,\n",
        "                    \"url_ficha_tecnica\": \"\"\n",
        "                }\n",
        "            else:\n",
        "                # modo completo: visitar página del producto\n",
        "                details = scrape_product_page(product_url)\n",
        "                prod = {\n",
        "                    \"sku\": details.get(\"sku\") or \"\",\n",
        "                    \"nombre\": details.get(\"nombre\") or name_block,\n",
        "                    \"categoria\": category_name,\n",
        "                    \"precio_sin_dcto\": details.get(\"precio_sin_dcto\") or \"\",\n",
        "                    \"precio_con_dcto\": details.get(\"precio_con_dcto\") or \"\",\n",
        "                    \"url\": product_url,\n",
        "                    \"url_ficha_tecnica\": details.get(\"url_ficha_tecnica\") or \"\"\n",
        "                }\n",
        "\n",
        "            # deduplicación por SKU preferente, sino por URL\n",
        "            key_sku = prod[\"sku\"] if prod[\"sku\"] else None\n",
        "            if key_sku and key_sku in progress.get(\"by_sku\", {}):\n",
        "                # duplicado por SKU -> ignorar\n",
        "                continue\n",
        "            if prod[\"url\"] in progress.get(\"by_url\", {}):\n",
        "                continue\n",
        "\n",
        "            # registrar en progreso y persistir poco a poco\n",
        "            idx = len(progress.get(\"collected\", [])) + 1\n",
        "            progress.setdefault(\"collected\", []).append(prod)\n",
        "            if prod[\"sku\"]:\n",
        "                progress.setdefault(\"by_sku\", {})[prod[\"sku\"]] = True\n",
        "            progress.setdefault(\"by_url\", {})[prod[\"url\"]] = True\n",
        "            progress[\"processed_count\"] = progress.get(\"processed_count\", 0) + 1\n",
        "\n",
        "            # persistir en DB\n",
        "            upsert_product_db(prod)\n",
        "\n",
        "            new_products.append(prod)\n",
        "\n",
        "            # Guardado incremental cada SAVE_EVERY_N\n",
        "            if progress[\"processed_count\"] % SAVE_EVERY_N == 0:\n",
        "                save_progress(progress)\n",
        "                # exportar parcial\n",
        "                df_partial = pd.DataFrame(progress[\"collected\"])\n",
        "                df_partial.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
        "                with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(progress[\"collected\"], f, ensure_ascii=False, indent=2)\n",
        "                print(f\"  [*] Guardado incremental: {progress['processed_count']} productos.\")\n",
        "\n",
        "        # detectar enlaces de paginación y añadir\n",
        "        pag_links = set()\n",
        "        for pag in soup.find_all(class_=re.compile(r'(pagination|paginador|page-numbers|nav-pages)', re.I)):\n",
        "            for a in pag.find_all('a', href=True):\n",
        "                pag_links.add(urljoin(page_url, a['href']))\n",
        "        for a in soup.find_all('a', href=True):\n",
        "            href = a['href']\n",
        "            if re.search(r'/page/\\d+/?', href) or re.search(r'(\\?|&)page=\\d+', href):\n",
        "                pag_links.add(urljoin(page_url, href))\n",
        "        for pl in pag_links:\n",
        "            if pl not in visited_pages:\n",
        "                to_visit_pages.append(pl)\n",
        "\n",
        "    if pages_crawled >= MAX_PAGES_PER_CATEGORY:\n",
        "        print(\"  [!] Límite de páginas por categoría alcanzado; detener para evitar loop infinito.\")\n",
        "\n",
        "    return new_products, progress"
      ],
      "metadata": {
        "id": "wiCSJQv06YcA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 10 - Orquestador principal: detectar categorías, cargar progreso, iterar y finalizar\n",
        "def export_all_formats(collected: List[Dict]):\n",
        "    # CSV\n",
        "    df = pd.DataFrame(collected, columns=[\"sku\",\"nombre\",\"categoria\",\"precio_sin_dcto\",\"precio_con_dcto\",\"url\",\"url_ficha_tecnica\"])\n",
        "    df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
        "    # JSON\n",
        "    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
        "        json.dump(collected, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Exportados: {OUTPUT_CSV}, {OUTPUT_JSON}\")\n",
        "    # SQLite ya fue actualizado incrementalmente (tabla productos)\n",
        "\n",
        "def main_run(limit_categories: Optional[int] = None, resume: bool = True):\n",
        "    # preparar DB y progreso\n",
        "    initialize_db()\n",
        "    progress = load_progress() if resume else {\"collected\": [], \"by_sku\": {}, \"by_url\": {}, \"last_category_index\": 0, \"processed_count\": 0}\n",
        "    # cargar home y detectar categorías\n",
        "    try:\n",
        "        home_html = fetch_html(BASE_URL)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] No se pudo acceder a {BASE_URL}: {e}\")\n",
        "        return []\n",
        "    cats = extract_header_categories(home_html)\n",
        "    if not cats:\n",
        "        # heurística alternativa\n",
        "        soup = BeautifulSoup(home_html, \"lxml\")\n",
        "        alt = []\n",
        "        for a in soup.find_all('a', href=True):\n",
        "            href = a['href']\n",
        "            if 'categoria' in href or 'product-category' in href or '/product' in href:\n",
        "                alt.append((clean_text(a.get_text()) or href, urljoin(BASE_URL, href)))\n",
        "        cats = alt[:50]\n",
        "    if limit_categories:\n",
        "        cats = cats[:limit_categories]\n",
        "    print(f\"Categorías detectadas: {len(cats)}\")\n",
        "    start_index = progress.get(\"last_category_index\", 0)\n",
        "    total_new = 0\n",
        "    for i, (name, url) in enumerate(cats):\n",
        "        if i < start_index:\n",
        "            continue\n",
        "        new_prods, progress = scrape_category(name, url, progress, start_id=len(progress.get(\"collected\", []))+1)\n",
        "        total_new += len(new_prods)\n",
        "        progress[\"last_category_index\"] = i + 1\n",
        "        # guardar progreso inmediato tras cada categoría\n",
        "        save_progress(progress)\n",
        "    # final export\n",
        "    export_all_formats(progress.get(\"collected\", []))\n",
        "    print(f\"Scraping completado. Nuevos productos: {total_new}. Total acumulado: {len(progress.get('collected', []))}\")\n",
        "    return progress.get(\"collected\", [])\n",
        "\n",
        "# Ejecutar\n",
        "collected = main_run(limit_categories=None, resume=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "fnBWYGwN6YZg",
        "outputId": "4b542c35-6392-40ca-e678-2e3a2928ff86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'List' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-448370437.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Celda 10 - Orquestador principal: detectar categorías, cargar progreso, iterar y finalizar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mexport_all_formats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollected\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sku\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"nombre\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"categoria\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"precio_sin_dcto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"precio_con_dcto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"url_ficha_tecnica\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_CSV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 11 - Mostrar resultados y primer vistazo\n",
        "import math\n",
        "print(f\"Total productos recolectados: {len(collected)}\")\n",
        "if collected:\n",
        "    df_final = pd.DataFrame(collected, columns=[\"sku\",\"nombre\",\"categoria\",\"precio_sin_dcto\",\"precio_con_dcto\",\"url\",\"url_ficha_tecnica\"])\n",
        "    display(df_final.head(50))\n",
        "else:\n",
        "    print(\"No se recogieron productos.\")"
      ],
      "metadata": {
        "id": "e4ITWgcJ6YW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notas finales y recomendaciones\n",
        "- Si la web carga productos vía JavaScript y ves pocos o ningún bloque en el HTML, dímelo y adaptamos para:\n",
        "  - a) investigar llamadas XHR (preferible) y replicarlas, o\n",
        "  - b) usar Selenium/Playwright (más pesado en Colab).\n",
        "- Puedes cambiar `FAST_MODE = True` al inicio para ejecutar rápidamente la lista sin visitar cada producto.\n",
        "- `progress.json` permite reanudar: no se volverá a procesar URLs/SKUs ya guardadas.\n",
        "- Si quieres, puedo:\n",
        "  - Añadir logging más detallado a archivo.\n",
        "  - Añadir descarga automática de fichas técnicas (PDF).\n",
        "  - Añadir multihilo seguro (con cuidado de no sobrecargar el servidor)."
      ],
      "metadata": {
        "id": "ZB4R-WYV6hwt"
      }
    }
  ]
}